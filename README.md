# Awesome Machine Learning Systems[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

## Algorithm Level(Efficient AI)

### Quantization

- **[2023-ICML]** [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2211.10438)
- **[2024-MLSys]** [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978) [[code](https://github.com/mit-han-lab/llm-awq?tab=readme-ov-file)]
- **[2024-NeurlPS]** [DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs](https://arxiv.org/pdf/2406.01721)
- **[2024-NeurlPS]** [QuaRot: QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs](https://arxiv.org/abs/2404.00456) [[code](https://github.com/spcl/QuaRot)]
- **[2024-ICLR]** [OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models](https://arxiv.org/abs/2308.13137) [[code](https://github.com/OpenGVLab/OmniQuant)]
- **[2024-MLSys]** [Atom: Low-Bit Quantization for Efficient and Accurate LLM Serving](https://arxiv.org/abs/2310.19102) [[code](https://github.com/efeslab/Atom)]
- **[2024-ECCV]** [MixDQ: Memory-Efficient Few-Step Text-to-Image Diffusion Models with Metric-Decoupled Mixed Precision Quantization](https://arxiv.org/abs/2405.17873) [[code](https://github.com/thu-nics/MixDQ)]
- **[2025.1]** [PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization](https://arxiv.org/abs/2410.05265) [[code](https://github.com/ChenMnZ/PrefixQuant)]
- **[2025-ICLR]** [SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models](https://arxiv.org/abs/2411.05007) [[code](https://github.com/mit-han-lab/nunchaku)]
- **[2025-ICLR]** [ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation](https://arxiv.org/abs/2406.02540) [[code](https://github.com/thu-nics/ViDiT-Q)]
- **[2025-ICLR]** [Rotated Runtime Smooth: Training-Free Activation Smoother for accurate INT4 inference](https://arxiv.org/abs/2409.20361)[[code](https://github.com/Coco58323/Rotated_Runtime_Smooth)]
- **[2025-ICLR]** [SpinQuant: LLM quantization with learned rotations](https://arxiv.org/abs/2405.16406) [[code](https://github.com/facebookresearch/SpinQuant)]
- **[2025-CVPR]** [Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers](https://arxiv.org/pdf/2406.17343) [[code](https://github.com/Juanerx/Q-DiT/)]
- **[2025-MLSys]** [QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving](https://arxiv.org/abs/2405.04532) [[code](https://github.com/mit-han-lab/omniserve)]

### Pruning

- **[2016-NeurlPS]** [Learning Structured Sparsity in Deep Neural Networks](https://proceedings.neurips.cc/paper/2016/hash/41bfd20a38bb1b0bec75acf0845530a7-Abstract.html)
- **[2017-ICCV]** [Channel Pruning for Accelerating Very Deep Neural Networks](https://openaccess.thecvf.com/content_iccv_2017/html/He_Channel_Pruning_for_ICCV_2017_paper.html)
- **[2021-ICLR]** [Neual Pruning via Growing Regularization](https://openreview.net/pdf?id=o966_Is_nPA)
- **[2021-CVPR]** [Convolutional Neural Network Pruning with Structural Redundancy Reduction](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Convolutional_Neural_Network_Pruning_With_Structural_Redundancy_Reduction_CVPR_2021_paper.pdf)
- **[2023.1]** [Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning](https://arxiv.org/abs/2301.05219)

## System Level

- **[2019.9]** [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) [[code](https://github.com/NVIDIA/Megatron-LM)]
- **[2023-SOSP]** [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180) [[code](https://github.com/vllm-project/vllm)]
- **[2024-MLSys]** [Atom: Low-Bit Quantization for Efficient and Accurate LLM Serving](https://arxiv.org/abs/2310.19102) [[code](https://github.com/efeslab/Atom)]
- **[2025-MLSys]** [QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving](https://arxiv.org/abs/2405.04532) [[code](https://github.com/mit-han-lab/omniserve)]

## RL System

- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
- [InstructGPT](https://arxiv.org/abs/2203.02155)