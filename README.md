# Awesome-Machine-Learning-Systems

## Algorithm Level

### Pruning

- **[2016-NeurlPS]** [Learning Structured Sparsity in Deep Neural Networks](https://proceedings.neurips.cc/paper/2016/hash/41bfd20a38bb1b0bec75acf0845530a7-Abstract.html)
- **[2017-ICCV]** [Channel Pruning for Accelerating Very Deep Neural Networks](https://openaccess.thecvf.com/content_iccv_2017/html/He_Channel_Pruning_for_ICCV_2017_paper.html)
- **[2021-ICLR]** [Neual Pruning via Growing Regularization](https://openreview.net/pdf?id=o966_Is_nPA)
- **[2021-CVPR]** [Convolutional Neural Network Pruning with Structural Redundancy Reduction](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Convolutional_Neural_Network_Pruning_With_Structural_Redundancy_Reduction_CVPR_2021_paper.pdf)
- **[2023.1]** [Why is the State of Neural Network Pruning so Confusing? On the Fairness, Comparison Setup, and Trainability in Network Pruning](https://arxiv.org/abs/2301.05219)

### Quantization

- **[2023-ICML]** [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2211.10438)
- **[2024-MLSys]** [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978) [[code](https://github.com/mit-han-lab/llm-awq?tab=readme-ov-file)]
- **[2024-NeurlPS]** [DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs](https://arxiv.org/pdf/2406.01721)
- **[2024-ICLR]** [OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models](https://arxiv.org/abs/2308.13137) [[code](https://github.com/OpenGVLab/OmniQuant)]
- **[2024-MLSys]** [Atom: Low-Bit Quantization for Efficient and Accurate LLM Serving](https://arxiv.org/abs/2310.19102) [[code](https://github.com/efeslab/Atom)]
- **[2025-ICLR]** [SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models](https://arxiv.org/abs/2411.05007) [[code](https://github.com/mit-han-lab/nunchaku)]

## System Level

- **[2023-SOSP]** [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180) [[code](https://github.com/vllm-project/vllm)]
- **[2019.9]** [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) [[code](https://github.com/NVIDIA/Megatron-LM)]
